## Hi there ðŸ‘‹

This is LCM-Lab, an open-source research team within the [OpenNLG Group](https://opennlg.cn/) that focuses on long-context modeling and optimization. Below is a list of our workâ€”please feel free to explore!

---

### ðŸ”¹ Long-Context Reward
1. **LongRM**: Pushing the limits of reward modeling beyond 128K tokens [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LongRM)  [![arXiv](https://img.shields.io/badge/arXiv-2510.06915-B31B1B?logo=arxiv)](https://arxiv.org/abs/2510.06915)

### ðŸ”¹ Long-Context Evaluation
1. **LOOM-Eval**: A comprehensive and efficient framework for long-context model evaluation  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LOOM-Eval)  [![arXiv](https://img.shields.io/badge/arXiv-2507.04723-B31B1B?logo=arxiv)](https://arxiv.org/abs/2507.04723)

2. **L-CiteEval** (ACL 2025): A faithfulness-oriented benchmark for long-context citation  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/L-CITEEVAL)  [![ACL Anthology](https://img.shields.io/badge/ACL-2025-4BA64B?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyIDEzLjVhMS41IDEuNSAwIDAgMCAxLjUtMS41IDEuNSAxLjUgMCAwIDAtMS41LTEuNSAxLjUgMS41IDAgMCAwLTEuNSAxLjUgMS41IDEuNSAwIDAgMCAxLjUgMS41em0wLTkuN2E5LjcgOS43IDAgMCAxIDkuNyA5LjcgOS43IDkuNyAwIDAgMS05LjcgOS43IDkuNyA5LjcgMCAwIDEtOS43LTkuNyA5LjcgOS43IDAgMCAxIDkuNy05Ljd6Ii8+PC9zdmc+)](https://aclanthology.org/2025.acl-long.263.pdf)

3. **MMLongCite**: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/jiqimaoke/MMLongCite)  [![arXiv](https://img.shields.io/badge/arXiv-2507.04723-B31B1B?logo=arxiv)](https://arxiv.org/abs/2510.13276)

### ðŸ”¹ Long-Context Modeling
1. **CDT (Context Denoising Training)**  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/context-denoising-training) [![arXiv](https://img.shields.io/badge/arXiv-2510.05862-B31B1B?logo=arxiv)](https://arxiv.org/abs/2510.05862)

2. **LOGO** (ICML 2025): Long cOntext aliGnment via efficient preference Optimization  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LOGO) [![ICML](https://img.shields.io/badge/ICML-2025-<color>?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyLDEyYTEwLDEwIDAgMSwwIDAsMTAgMTAsMTAgMCAwLDAtMTB6Ii8+PHBhdGggZmlsbD0iIzAwMCIgZD0iTTEyLDExLjVhLjUsLjUgMCAwLDEgLjUsLjV2MWgtMXYtMS41eiIvPjwvc3ZnPg==)](https://openreview.net/pdf?id=vVEBtDDSA6)

3. **Global-Mamba** (ACL 2025): Efficient long-context modeling architecture  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/Global_Mamba)  [![ACL Anthology](https://img.shields.io/badge/ACL-2025-4BA64B?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyIDEzLjVhMS41IDEuNSAwIDAgMCAxLjUtMS41IDEuNSAxLjUgMCAwIDAtMS41LTEuNSAxLjUgMS41IDAgMCAwLTEuNSAxLjUgMS41IDEuNSAwIDAgMCAxLjUgMS41em0wLTkuN2E5LjcgOS43IDAgMCAxIDkuNyA5LjcgOS43IDkuNyAwIDAgMS05LjcgOS43IDkuNyA5LjcgMCAwIDEtOS43LTkuNyA5LjcgOS43IDAgMCAxIDkuNy05Ljd6Ii8+PC9zdmc+)](https://aclanthology.org/2025.findings-acl.629.pdf)

---

If you have any questions about the code or paper details, please donâ€™t hesitate to open an issue or contact us directly at zecheng.tang@foxmail.com .

<!--

**Here are some ideas to get you started:**

ðŸ™‹â€â™€ï¸ A short introduction - what is your organization all about?
ðŸŒˆ Contribution guidelines - how can the community get involved?
ðŸ‘©â€ðŸ’» Useful resources - where can the community find your docs? Is there anything else the community should know?
ðŸ¿ Fun facts - what does your team eat for breakfast?
ðŸ§™ Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->
