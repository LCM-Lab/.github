## Hi there ðŸ‘‹

This is LCM-Lab, an open-source research team within the [OpenNLG Group](https://opennlg.cn/) that focuses on long-context modeling and optimization. Below is a list of our workâ€”please feel free to explore!

If you have any questions about the code or paper details, please donâ€™t hesitate to open an issue or contact this email [zecheng.tang@foxmail.com](zecheng.tang@foxmail.com).

---

### ðŸ”¹ Long-Context Reward
- **LongRM**: Pushing the limits of reward modeling beyond 128K tokens <br>
  [![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LongRM)  [![arXiv](https://img.shields.io/badge/arXiv-2510.06915-B31B1B?logo=arxiv)](https://arxiv.org/abs/2510.06915)

- **MemoryRewardBench**: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/MemRewardBench)  [![arXiv](https://img.shields.io/badge/arXiv-2601.11969-B31B1B?logo=arxiv)](https://arxiv.org/abs/2601.11969)

### ðŸ”¹ Long-Context Evaluation
- **LOOM-Eval**: A comprehensive and efficient framework for long-context model evaluation <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LOOM-Eval)  [![arXiv](https://img.shields.io/badge/arXiv-2507.04723-B31B1B?logo=arxiv)](https://arxiv.org/abs/2507.04723)

- **L-CiteEval** (ACL 2025): A faithfulness-oriented benchmark for long-context citation <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/L-CITEEVAL)  [![ACL Anthology](https://img.shields.io/badge/ACL-2025-4BA64B?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyIDEzLjVhMS41IDEuNSAwIDAgMCAxLjUtMS41IDEuNSAxLjUgMCAwIDAtMS41LTEuNSAxLjUgMS41IDAgMCAwLTEuNSAxLjUgMS41IDEuNSAwIDAgMCAxLjUgMS41em0wLTkuN2E5LjcgOS43IDAgMCAxIDkuNyA5LjcgOS43IDkuNyAwIDAgMS05LjcgOS43IDkuNyA5LjcgMCAwIDEtOS43LTkuNyA5LjcgOS43IDAgMCAxIDkuNy05Ljd6Ii8+PC9zdmc+)](https://aclanthology.org/2025.acl-long.263.pdf)

- **MMLongCite**: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/jiqimaoke/MMLongCite)  [![arXiv](https://img.shields.io/badge/arXiv-2507.04723-B31B1B?logo=arxiv)](https://arxiv.org/abs/2601.17367)

### ðŸ”¹ Long-Context Modeling
- **Elastic Attention**: Test-time Adaptive Sparsity Ratios for Efficient Transformers <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/Elastic-Attention) [![arXiv](https://img.shields.io/badge/arXiv-2510.05862-B31B1B?logo=arxiv)](https://arxiv.org/abs/2510.05862)

- **CDT** (ICLR 2026): Context Denoising Training for Long-Context Modeling <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/context-denoising-training) [![ICLR](https://img.shields.io/badge/ICLR-2026-<color>?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyLDEyYTEwLDEwIDAgMSwwIDAsMTAgMTAsMTAgMCAwLDAtMTB6Ii8+PHBhdGggZmlsbD0iIzAwMCIgZD0iTTEyLDExLjVhLjUsLjUgMCAwLDEgLjUsLjV2MWgtMXYtMS41eiIvPjwvc3ZnPg==)](https://openreview.net/pdf?id=xvGyyh6MG7)

- **LOGO** (ICML 2025): Long cOntext aliGnment via efficient preference Optimization <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/LOGO) [![ICML](https://img.shields.io/badge/ICML-2025-<color>?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyLDEyYTEwLDEwIDAgMSwwIDAsMTAgMTAsMTAgMCAwLDAtMTB6Ii8+PHBhdGggZmlsbD0iIzAwMCIgZD0iTTEyLDExLjVhLjUsLjUgMCAwLDEgLjUsLjV2MWgtMXYtMS41eiIvPjwvc3ZnPg==)](https://openreview.net/pdf?id=vVEBtDDSA6)

- **Global-Mamba** (ACL 2025): Efficient long-context modeling architecture <br>
[![GitHub](https://img.shields.io/badge/GitHub-Code-181717?logo=github)](https://github.com/LCM-Lab/Global_Mamba)  [![ACL Anthology](https://img.shields.io/badge/ACL-2025-4BA64B?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI+PHBhdGggZmlsbD0iI2ZmZiIgZD0iTTEyIDEzLjVhMS41IDEuNSAwIDAgMCAxLjUtMS41IDEuNSAxLjUgMCAwIDAtMS41LTEuNSAxLjUgMS41IDAgMCAwLTEuNSAxLjUgMS41IDEuNSAwIDAgMCAxLjUgMS41em0wLTkuN2E5LjcgOS43IDAgMCAxIDkuNyA5LjcgOS43IDkuNyAwIDAgMS05LjcgOS43IDkuNyA5LjcgMCAwIDEtOS43LTkuNyA5LjcgOS43IDAgMCAxIDkuNy05Ljd6Ii8+PC9zdmc+)](https://aclanthology.org/2025.findings-acl.629.pdf)
